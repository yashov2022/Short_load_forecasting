{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNIu+fbVb7BrRPLb111pAPk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"foTV80WGeb0E"},"outputs":[],"source":["import numpy as np#model1\n","import pandas as pd\n","from sklearn.svm import SVR\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","file_path = '/content/morning cluster.sx.xlsx'  # Update this with your actual file path\n","data = pd.read_excel(file_path)\n","data['Load_Lag_1'] = data['POWER (KW)'].shift(1)     # Previous hour\n","data['Load_Lag_24'] = data['POWER (KW)'].shift(24)   # Previous day\n","data['Load_Lag_168'] = data['POWER (KW)'].shift(168) # Previous week\n","\n","# Create lagged features for seasonal parameters (example: Temperature and Humidity)\n","data['Temp_Lag_1'] = data['Temp (F)'].shift(1)\n","data['Temp_Lag_24'] = data['Temp (F)'].shift(24)\n","data['Humidity_Lag_1'] = data['Humidity (%)'].shift(1)\n","data['Humidity_Lag_24'] = data['Humidity (%)'].shift(24)\n","data.dropna(inplace=True)\n","X = data[['Load_Lag_1','Load_Lag_24','Load_Lag_168','Temp_Lag_1','\"WEEKEND/WEEKDAY\"','SEASON','Temp_Lag_24','Humidity_Lag_1','Humidity_Lag_24']]\n","y = data['POWER (KW)']\n","poly = PolynomialFeatures(degree=2, include_bias=False)\n","X_poly = poly.fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","scaler_X = MinMaxScaler()\n","X_train_scaled = scaler_X.fit_transform(X_train)\n","X_test_scaled = scaler_X.transform(X_test)\n","scaler_y = MinMaxScaler()\n","y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n","y_test_scaled_morning = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n","param_grid = [\n","    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]},\n","    {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto'], 'coef0': [0, 1]},\n","    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1, 0.01]},\n","    {'kernel': ['sigmoid'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'coef0': [0, 1]}\n","]\n","\n","# Initialize the SVR model\n","svr = SVR()\n","grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error')\n","\n","# Fit the model\n","grid_search.fit(X_train_scaled, y_train_scaled)\n","\n","# Get the best estimator\n","best_svr = grid_search.best_estimator_\n","y_pred_train_scaled_morning = best_svr.predict(X_train_scaled)\n","y_pred_test_scaled_morning = best_svr.predict(X_test_scaled)"]},{"cell_type":"code","source":["import pandas as pd  #model2\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import matplotlib.pyplot as plt\n","import math\n","data = pd.read_excel('/content/afternoon cluster.xlsx')\n","# Select features for the model\n","features = ['POWER (KW)', 'Temp (F)', 'Humidity (%)', '\"WEEKEND/WEEKDAY\"','SEASON']\n","# Scale the features\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled_data = scaler.fit_transform(data[features])\n","\n","# Convert the scaled data back to a DataFrame\n","scaled_data = pd.DataFrame(scaled_data, columns=features, index=data.index)\n","def create_sequences(data, seq_length):\n","    xs = []\n","    ys = []\n","    for i in range(len(data) - seq_length):\n","        x = data[i:i + seq_length]\n","        y = data.iloc[i + seq_length, 0]  # Load is the target variable\n","        xs.append(x)\n","        ys.append(y)\n","    return np.array(xs), np.array(ys)\n","\n","seq_length = 24  # Using 24 hours of data to predict the next hour's load\n","X, y = create_sequences(scaled_data, seq_length)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","model = Sequential()\n","model.add(Bidirectional(LSTM(100, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Bidirectional(LSTM(100, return_sequences=False)))\n","model.add(Dropout(0.2))\n","model.add(Dense(50))\n","model.add(Dense(1))\n","\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","history = model.fit(X_train, y_train, batch_size=15, epochs=30, validation_split=0.2)\n","predictions_afternoon = model.predict(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygPwjmaEeu46","executionInfo":{"status":"ok","timestamp":1719633938499,"user_tz":-330,"elapsed":157358,"user":{"displayName":"Yashovardhan A","userId":"10696252340457080711"}},"outputId":"f14c9167-8e35-4dba-f7fb-39846d075592"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","62/62 [==============================] - 13s 82ms/step - loss: 0.0214 - val_loss: 0.0120\n","Epoch 2/30\n","62/62 [==============================] - 3s 55ms/step - loss: 0.0112 - val_loss: 0.0102\n","Epoch 3/30\n","62/62 [==============================] - 5s 81ms/step - loss: 0.0097 - val_loss: 0.0087\n","Epoch 4/30\n","62/62 [==============================] - 3s 56ms/step - loss: 0.0096 - val_loss: 0.0092\n","Epoch 5/30\n","62/62 [==============================] - 3s 54ms/step - loss: 0.0099 - val_loss: 0.0080\n","Epoch 6/30\n","62/62 [==============================] - 4s 63ms/step - loss: 0.0082 - val_loss: 0.0085\n","Epoch 7/30\n","62/62 [==============================] - 4s 70ms/step - loss: 0.0075 - val_loss: 0.0082\n","Epoch 8/30\n","62/62 [==============================] - 3s 53ms/step - loss: 0.0077 - val_loss: 0.0077\n","Epoch 9/30\n","62/62 [==============================] - 3s 53ms/step - loss: 0.0078 - val_loss: 0.0079\n","Epoch 10/30\n","62/62 [==============================] - 5s 83ms/step - loss: 0.0079 - val_loss: 0.0079\n","Epoch 11/30\n","62/62 [==============================] - 3s 54ms/step - loss: 0.0076 - val_loss: 0.0087\n","Epoch 12/30\n","62/62 [==============================] - 4s 73ms/step - loss: 0.0077 - val_loss: 0.0077\n","Epoch 13/30\n","62/62 [==============================] - 6s 91ms/step - loss: 0.0071 - val_loss: 0.0073\n","Epoch 14/30\n","62/62 [==============================] - 3s 54ms/step - loss: 0.0071 - val_loss: 0.0074\n","Epoch 15/30\n","62/62 [==============================] - 3s 54ms/step - loss: 0.0067 - val_loss: 0.0084\n","Epoch 16/30\n","62/62 [==============================] - 4s 58ms/step - loss: 0.0077 - val_loss: 0.0071\n","Epoch 17/30\n","62/62 [==============================] - 5s 77ms/step - loss: 0.0066 - val_loss: 0.0077\n","Epoch 18/30\n","62/62 [==============================] - 3s 56ms/step - loss: 0.0073 - val_loss: 0.0085\n","Epoch 19/30\n","62/62 [==============================] - 3s 55ms/step - loss: 0.0070 - val_loss: 0.0079\n","Epoch 20/30\n","62/62 [==============================] - 5s 81ms/step - loss: 0.0063 - val_loss: 0.0076\n","Epoch 21/30\n","62/62 [==============================] - 3s 55ms/step - loss: 0.0072 - val_loss: 0.0073\n","Epoch 22/30\n","62/62 [==============================] - 3s 55ms/step - loss: 0.0065 - val_loss: 0.0069\n","Epoch 23/30\n","62/62 [==============================] - 4s 59ms/step - loss: 0.0067 - val_loss: 0.0073\n","Epoch 24/30\n","62/62 [==============================] - 5s 78ms/step - loss: 0.0066 - val_loss: 0.0073\n","Epoch 25/30\n","62/62 [==============================] - 3s 55ms/step - loss: 0.0069 - val_loss: 0.0079\n","Epoch 26/30\n","62/62 [==============================] - 3s 54ms/step - loss: 0.0066 - val_loss: 0.0072\n","Epoch 27/30\n","62/62 [==============================] - 5s 84ms/step - loss: 0.0064 - val_loss: 0.0069\n","Epoch 28/30\n","62/62 [==============================] - 3s 56ms/step - loss: 0.0065 - val_loss: 0.0077\n","Epoch 29/30\n","62/62 [==============================] - 4s 57ms/step - loss: 0.0063 - val_loss: 0.0075\n","Epoch 30/30\n","62/62 [==============================] - 4s 65ms/step - loss: 0.0064 - val_loss: 0.0076\n","9/9 [==============================] - 2s 47ms/step\n"]}]},{"cell_type":"code","source":["import pandas as pd  #model3\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import train_test_split\n","file_path = '/content/evening clusterxlsx.xlsx'  # replace with your file path\n","data = pd.read_excel(file_path)\n","data['Load_Lag_1'] = data['POWER (KW)'].shift(1)     # Previous hour\n","data['Load_Lag_24'] = data['POWER (KW)'].shift(24)   # Previous day\n","data['Load_Lag_168'] = data['POWER (KW)'].shift(168) # Previous week\n","\n","# Create lagged features for seasonal parameters (example: Temperature and Humidity)\n","data['Temp_Lag_1'] = data['Temp (F)'].shift(1)\n","data['Temp_Lag_24'] = data['Temp (F)'].shift(24)\n","data['Humidity_Lag_1'] = data['Humidity (%)'].shift(1)\n","data['Humidity_Lag_24'] = data['Humidity (%)'].shift(24)\n","data.dropna(inplace=True)\n","features = ['Load_Lag_1', 'Load_Lag_24', 'Load_Lag_168', 'Temp_Lag_1', 'Temp_Lag_24', 'Humidity_Lag_1', 'Humidity_Lag_24','\"WEEKEND/WEEKDAY\"', 'SEASON']\n","# Scale the features\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled_data = scaler.fit_transform(data[features])\n","\n","# Convert the scaled data back to a DataFrame\n","scaled_data = pd.DataFrame(scaled_data, columns=features, index=data.index)\n","def create_sequences(data, seq_length):\n","    xs = []\n","    ys = []\n","    for i in range(len(data) - seq_length):\n","        x = data[i:i + seq_length]\n","        y = data.iloc[i + seq_length, 0]  # Load is the target variable\n","        xs.append(x)\n","        ys.append(y)\n","    return np.array(xs), np.array(ys)\n","\n","seq_length = 24  # Using 24 hours of data to predict the next hour's load\n","X, y = create_sequences(scaled_data, seq_length)\n","X_train, X_test, y_train, y_test_eveing = train_test_split(X, y, test_size=0.2, random_state=42)\n","model = Sequential()\n","model.add(Bidirectional(LSTM(100, return_sequences=True), input_shape=(seq_length, X_train.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Bidirectional(LSTM(100, return_sequences=False)))\n","model.add(Dropout(0.2))\n","model.add(Dense(50))\n","model.add(Dense(1))\n","\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","history = model.fit(X_train, y_train, batch_size=15, epochs=30, validation_split=0.2)\n","predictions_evening = model.predict(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Gum4avHgBNo","executionInfo":{"status":"ok","timestamp":1719634099465,"user_tz":-330,"elapsed":153625,"user":{"displayName":"Yashovardhan A","userId":"10696252340457080711"}},"outputId":"1f6ad038-7c81-48ac-f70b-b659d591ad8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","55/55 [==============================] - 12s 83ms/step - loss: 0.0490 - val_loss: 0.0178\n","Epoch 2/30\n","55/55 [==============================] - 4s 74ms/step - loss: 0.0222 - val_loss: 0.0131\n","Epoch 3/30\n","55/55 [==============================] - 3s 59ms/step - loss: 0.0182 - val_loss: 0.0143\n","Epoch 4/30\n","55/55 [==============================] - 3s 53ms/step - loss: 0.0168 - val_loss: 0.0120\n","Epoch 5/30\n","55/55 [==============================] - 3s 54ms/step - loss: 0.0153 - val_loss: 0.0106\n","Epoch 6/30\n","55/55 [==============================] - 4s 76ms/step - loss: 0.0159 - val_loss: 0.0107\n","Epoch 7/30\n","55/55 [==============================] - 3s 62ms/step - loss: 0.0160 - val_loss: 0.0106\n","Epoch 8/30\n","55/55 [==============================] - 3s 56ms/step - loss: 0.0153 - val_loss: 0.0106\n","Epoch 9/30\n","55/55 [==============================] - 3s 56ms/step - loss: 0.0148 - val_loss: 0.0115\n","Epoch 10/30\n","55/55 [==============================] - 4s 78ms/step - loss: 0.0137 - val_loss: 0.0111\n","Epoch 11/30\n","55/55 [==============================] - 3s 62ms/step - loss: 0.0141 - val_loss: 0.0117\n","Epoch 12/30\n","55/55 [==============================] - 3s 55ms/step - loss: 0.0135 - val_loss: 0.0098\n","Epoch 13/30\n","55/55 [==============================] - 3s 55ms/step - loss: 0.0128 - val_loss: 0.0106\n","Epoch 14/30\n","55/55 [==============================] - 4s 78ms/step - loss: 0.0134 - val_loss: 0.0147\n","Epoch 15/30\n","55/55 [==============================] - 3s 62ms/step - loss: 0.0176 - val_loss: 0.0110\n","Epoch 16/30\n","55/55 [==============================] - 3s 54ms/step - loss: 0.0143 - val_loss: 0.0098\n","Epoch 17/30\n","55/55 [==============================] - 3s 56ms/step - loss: 0.0135 - val_loss: 0.0124\n","Epoch 18/30\n","55/55 [==============================] - 5s 83ms/step - loss: 0.0126 - val_loss: 0.0116\n","Epoch 19/30\n","55/55 [==============================] - 3s 55ms/step - loss: 0.0136 - val_loss: 0.0110\n","Epoch 20/30\n","55/55 [==============================] - 3s 55ms/step - loss: 0.0127 - val_loss: 0.0102\n","Epoch 21/30\n","55/55 [==============================] - 3s 55ms/step - loss: 0.0140 - val_loss: 0.0102\n","Epoch 22/30\n","55/55 [==============================] - 5s 84ms/step - loss: 0.0131 - val_loss: 0.0109\n","Epoch 23/30\n","55/55 [==============================] - 3s 55ms/step - loss: 0.0122 - val_loss: 0.0104\n","Epoch 24/30\n","55/55 [==============================] - 3s 55ms/step - loss: 0.0129 - val_loss: 0.0103\n","Epoch 25/30\n","55/55 [==============================] - 3s 55ms/step - loss: 0.0122 - val_loss: 0.0094\n","Epoch 26/30\n","55/55 [==============================] - 5s 83ms/step - loss: 0.0122 - val_loss: 0.0104\n","Epoch 27/30\n","55/55 [==============================] - 4s 81ms/step - loss: 0.0120 - val_loss: 0.0093\n","Epoch 28/30\n","55/55 [==============================] - 3s 59ms/step - loss: 0.0125 - val_loss: 0.0109\n","Epoch 29/30\n","55/55 [==============================] - 4s 75ms/step - loss: 0.0124 - val_loss: 0.0095\n","Epoch 30/30\n","55/55 [==============================] - 4s 65ms/step - loss: 0.0119 - val_loss: 0.0095\n","8/8 [==============================] - 2s 24ms/step\n"]}]},{"cell_type":"code","source":["import numpy as np#model1\n","import pandas as pd\n","from sklearn.svm import SVR\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","file_path = '/content/night cluster.xlsx'  # Update this with your actual file path\n","data = pd.read_excel(file_path)\n","data['Load_Lag_1'] = data['POWER (KW)'].shift(1)     # Previous hour\n","data['Load_Lag_24'] = data['POWER (KW)'].shift(24)   # Previous day\n","data['Load_Lag_168'] = data['POWER (KW)'].shift(168) # Previous week\n","\n","# Create lagged features for seasonal parameters (example: Temperature and Humidity)\n","data['Temp_Lag_1'] = data['Temp (F)'].shift(1)\n","data['Temp_Lag_24'] = data['Temp (F)'].shift(24)\n","data['Humidity_Lag_1'] = data['Humidity (%)'].shift(1)\n","data['Humidity_Lag_24'] = data['Humidity (%)'].shift(24)\n","data.dropna(inplace=True)\n","X = data[['Load_Lag_1','Load_Lag_24','Load_Lag_168','Temp_Lag_1','\"WEEKEND/WEEKDAY\"','SEASON','Temp_Lag_24','Humidity_Lag_1','Humidity_Lag_24']]\n","y = data['POWER (KW)']\n","poly = PolynomialFeatures(degree=2, include_bias=False)\n","X_poly = poly.fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","scaler_X = MinMaxScaler()\n","X_train_scaled = scaler_X.fit_transform(X_train)\n","X_test_scaled = scaler_X.transform(X_test)\n","scaler_y = MinMaxScaler()\n","y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n","y_test_scaled_morning = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n","param_grid = [\n","    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]},\n","    {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto'], 'coef0': [0, 1]},\n","    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1, 0.01]},\n","    {'kernel': ['sigmoid'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'coef0': [0, 1]}\n","]\n","\n","# Initialize the SVR model\n","svr = SVR()\n","grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error')\n","\n","# Fit the model\n","grid_search.fit(X_train_scaled, y_train_scaled)\n","\n","# Get the best estimator\n","best_svr = grid_search.best_estimator_\n","y_pred_train_scaled_morning = best_svr.predict(X_train_scaled)\n","y_pred_test_scaled_morning = best_svr.predict(X_test_scaled)\n","y_pred_test_scaled_night = best_svr.predict(X_test_scaled)"],"metadata":{"id":"CF40bgLzh1Ut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.svm import SVR\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","file_path = '/content/godishala clusters removed.xlsx' # Update this with your actual file path\n","data = pd.read_excel(file_path)\n","X = data[['Temp (F)', 'Humidity (%)', '\"WEEKEND/WEEKDAY\"','SEASON']]\n","y = data['POWER (KW)']\n","poly = PolynomialFeatures(degree=2, include_bias=False)\n","X_poly = poly.fit_transform(X)\n","X_train, X_test, y_train, y_test_nocluster = train_test_split(X, y, test_size=0.2, random_state=0)\n","scaler_X = MinMaxScaler()\n","X_train_scaled = scaler_X.fit_transform(X_train)\n","X_test_scaled = scaler_X.transform(X_test)\n","scaler_y = MinMaxScaler()\n","y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n","y_test_scaled_nonclusterd = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n","param_grid = [\n","    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]},\n","    {'kernel': ['poly'], 'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto'], 'coef0': [0, 1]},\n","    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1, 0.01]},\n","    {'kernel': ['sigmoid'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'coef0': [0, 1]}\n","]\n","\n","# Initialize the SVR model\n","svr = SVR()\n","grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error')\n","\n","# Fit the model\n","grid_search.fit(X_train_scaled, y_train_scaled)\n","\n","# Get the best estimator\n","best_svr = grid_search.best_estimator_\n","y_pred_train_scaled = best_svr.predict(X_train_scaled)\n","y_pred_test_scaled_nonclusterd_noclusterd= best_svr.predict(X_test_scaled)\n"],"metadata":{"id":"XAQ1Kv6jiYl4"},"execution_count":null,"outputs":[]}]}